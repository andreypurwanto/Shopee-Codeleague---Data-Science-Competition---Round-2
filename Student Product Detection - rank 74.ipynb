{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of shopee_codeleague_image_classification_rank74.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hPfY7Z86SFqR",
        "W9tNs0fGz05g",
        "5Ra0KBY8zjnT",
        "NA5v8FpQzoq8",
        "bcef9xK_ztWi",
        "VK6BqsvLFqDR",
        "6OPecPSuF56z",
        "PLlIw-tFzXLW",
        "XuHftuh_HIlU",
        "Eqf-ZjA2OOsG",
        "gNeYmMclO9hr",
        "aq5EKv5TP31t"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPfY7Z86SFqR",
        "colab_type": "text"
      },
      "source": [
        "#README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQzjFG9GSNOi",
        "colab_type": "text"
      },
      "source": [
        "This is a very long code to run, in the process of making it, we save the code the run it again. But all of the code should be able to run in a single run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9tNs0fGz05g",
        "colab_type": "text"
      },
      "source": [
        "#UNZIP (WE USE GOOGLE COLAB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgQFaFSEldrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a59c508b-76cd-4a42-c493-d7c57b80d121"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaOBpHMImQPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/drive/My Drive/Copy of shopee-product-detection-dataset.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA5v8FpQzoq8",
        "colab_type": "text"
      },
      "source": [
        "#IMAGE AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn9yXIgbo5Yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "4005314f-0c75-4350-a303-fca29fb5b125"
      },
      "source": [
        "pip install albumentations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.6/dist-packages (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.18.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from albumentations) (4.1.2.30)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.12.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.0.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=8c9a0700d88a28c05cb9cf44b1d56a713a2cefa8556a0c4a51e598e1d9d08948\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgECHKWgo4e8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "92348203-6141-4916-fb39-9238dfc18ede"
      },
      "source": [
        "pip install git+https://github.com/mjkvaak/ImageDataAugmentor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/mjkvaak/ImageDataAugmentor\n",
            "  Cloning https://github.com/mjkvaak/ImageDataAugmentor to /tmp/pip-req-build-268_3jv0\n",
            "  Running command git clone -q https://github.com/mjkvaak/ImageDataAugmentor /tmp/pip-req-build-268_3jv0\n",
            "Collecting opencv-python>=4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/c2/e9cf54ae5b1102020ef895866a67cb2e1aef72f16dd1fde5b5fb1495ad9c/opencv_python-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (28.2MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2MB 111kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ImageDataAugmentor==0.0.0) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from ImageDataAugmentor==0.0.0) (7.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from ImageDataAugmentor==0.0.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python>=4.2->ImageDataAugmentor==0.0.0) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->ImageDataAugmentor==0.0.0) (1.12.0)\n",
            "Building wheels for collected packages: ImageDataAugmentor\n",
            "  Building wheel for ImageDataAugmentor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ImageDataAugmentor: filename=ImageDataAugmentor-0.0.0-cp36-none-any.whl size=28017 sha256=a5e9a009f96d88a66cd7224a506ab09db353f8dee78f3a9fde696c3e21243bfd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h9011cww/wheels/d9/10/55/6fca35a4072f87d694876d56ece64db3846cf45e1da1c381fe\n",
            "Successfully built ImageDataAugmentor\n",
            "Installing collected packages: opencv-python, ImageDataAugmentor\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "Successfully installed ImageDataAugmentor-0.0.0 opencv-python-4.2.0.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjbUIXMVo7Ww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8432321f-5d17-4617-df65-6105be9e0cd3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from ImageDataAugmentor.image_data_augmentor import *\n",
        "import albumentations\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense,Flatten,Dropout,Concatenate,GlobalAveragePooling2D,Lambda,ZeroPadding2D\n",
        "from tensorflow.keras.layers import SeparableConv2D,BatchNormalization,MaxPooling2D,Conv2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUl6NH4So9oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUGMENTATIONS = albumentations.Compose([\n",
        "    #albumentations.Transpose(p=0.5),\n",
        "    albumentations.HorizontalFlip(p=0.5),\n",
        "    albumentations.CLAHE(),\n",
        "    albumentations.Cutout(num_holes=2,max_h_size=70,max_w_size=70,p=0.2),\n",
        "    albumentations.OneOf([\n",
        "    albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n",
        "    albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n",
        "    ],p=1),\n",
        "    albumentations.Blur(p=0.05),\n",
        "    albumentations.HueSaturationValue(p=0.5),\n",
        "    albumentations.RGBShift(p=0.5),\n",
        "    albumentations.Rotate(limit=15),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYlH6Oaro_gm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7fa662ee-5d3a-45f9-e6ac-da5c2090b92c"
      },
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment = AUGMENTATIONS,\n",
        "        preprocess_input=None,\n",
        "        validation_split=0.1)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 94869 images belonging to 42 classes.\n",
            "Found 10523 images belonging to 42 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ra0KBY8zjnT",
        "colab_type": "text"
      },
      "source": [
        "#DEFINE MODEL 1: INCEPTIONRESNETv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW3zB2YrxJfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = tf.keras.applications.InceptionResNetV2(\n",
        "    input_shape=None,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling='avg',\n",
        "    classes=1000,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKgmq3xkxW1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.trainable=False\n",
        "model_in = Input(shape=(299,299,3))\n",
        "x = base_model(model_in,training=False)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "model_out = layers.Dense(42,activation='softmax')(x)\n",
        "model = Model(model_in,model_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8TSZDRbvidd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e6c3b15f-5564-4937-921a-a72930a1acfc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_resnet_v2 (Model)  (None, 1536)              54336736  \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 42)                64554     \n",
            "=================================================================\n",
            "Total params: 54,401,290\n",
            "Trainable params: 64,554\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcef9xK_ztWi",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN MODEL 1: RESNETV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fuW65DyP78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.01\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='percobaan_kaggle_v3_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "#we dont use LR auto searcher (because the error that we encountered before), so we tune the LR by ourselves"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckiV9OUFyUQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#log for this epoch is deleted, because we keep train save load our model.\n",
        "epochs = 10\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttIr9JVxydG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='percobaan_kaggle_v3_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "#we dont use LR auto searcher (because the error that we encountered before), so we tune the LR by ourselves"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS15oIPnygHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#log for this epoch is deleted, because we keep train save load our model.\n",
        "epochs = 10\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIQHUigmwPKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.0001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='percobaan_kaggle_v3_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeKzYS_qpVfj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a4e7a7b-0f5f-40bc-d498-ac716a0459d5"
      },
      "source": [
        "#we dont use LR auto searcher (because the error that we encountered before), so we tune the LR by ourselves\n",
        "epochs = 20\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9462 - accuracy: 0.7368\n",
            "Epoch 00001: val_loss improved from inf to 1.05149, saving model to percobaan_kaggle_v3_checkpoint.hdf5\n",
            "2965/2965 [==============================] - 1727s 582ms/step - loss: 0.9462 - accuracy: 0.7368 - val_loss: 1.0515 - val_accuracy: 0.7172\n",
            "Epoch 2/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9432 - accuracy: 0.7381\n",
            "Epoch 00002: val_loss improved from 1.05149 to 1.04810, saving model to percobaan_kaggle_v3_checkpoint.hdf5\n",
            "2965/2965 [==============================] - 1679s 566ms/step - loss: 0.9432 - accuracy: 0.7381 - val_loss: 1.0481 - val_accuracy: 0.7123\n",
            "Epoch 3/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9419 - accuracy: 0.7377\n",
            "Epoch 00003: val_loss improved from 1.04810 to 1.04387, saving model to percobaan_kaggle_v3_checkpoint.hdf5\n",
            "2965/2965 [==============================] - 1679s 566ms/step - loss: 0.9419 - accuracy: 0.7377 - val_loss: 1.0439 - val_accuracy: 0.7179\n",
            "Epoch 4/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9401 - accuracy: 0.7394\n",
            "Epoch 00004: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1690s 570ms/step - loss: 0.9401 - accuracy: 0.7394 - val_loss: 1.0513 - val_accuracy: 0.7134\n",
            "Epoch 5/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9393 - accuracy: 0.7384\n",
            "Epoch 00005: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1698s 573ms/step - loss: 0.9393 - accuracy: 0.7384 - val_loss: 1.0540 - val_accuracy: 0.7143\n",
            "Epoch 6/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9375 - accuracy: 0.7397\n",
            "Epoch 00006: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1741s 587ms/step - loss: 0.9375 - accuracy: 0.7397 - val_loss: 1.0520 - val_accuracy: 0.7142\n",
            "Epoch 7/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9354 - accuracy: 0.7398\n",
            "Epoch 00007: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1761s 594ms/step - loss: 0.9354 - accuracy: 0.7398 - val_loss: 1.0516 - val_accuracy: 0.7125\n",
            "Epoch 8/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9353 - accuracy: 0.7400\n",
            "Epoch 00008: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1757s 593ms/step - loss: 0.9353 - accuracy: 0.7400 - val_loss: 1.0494 - val_accuracy: 0.7163\n",
            "Epoch 9/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9351 - accuracy: 0.7404\n",
            "Epoch 00009: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1762s 594ms/step - loss: 0.9351 - accuracy: 0.7404 - val_loss: 1.0479 - val_accuracy: 0.7155\n",
            "Epoch 10/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9325 - accuracy: 0.7407\n",
            "Epoch 00010: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1764s 595ms/step - loss: 0.9325 - accuracy: 0.7407 - val_loss: 1.0501 - val_accuracy: 0.7152\n",
            "Epoch 11/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9309 - accuracy: 0.7410\n",
            "Epoch 00011: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1786s 602ms/step - loss: 0.9309 - accuracy: 0.7410 - val_loss: 1.0445 - val_accuracy: 0.7187\n",
            "Epoch 12/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9291 - accuracy: 0.7409\n",
            "Epoch 00012: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1779s 600ms/step - loss: 0.9291 - accuracy: 0.7409 - val_loss: 1.0568 - val_accuracy: 0.7118\n",
            "Epoch 13/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9292 - accuracy: 0.7417\n",
            "Epoch 00013: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1768s 596ms/step - loss: 0.9292 - accuracy: 0.7417 - val_loss: 1.0484 - val_accuracy: 0.7148\n",
            "Epoch 14/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9301 - accuracy: 0.7415\n",
            "Epoch 00014: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1777s 599ms/step - loss: 0.9301 - accuracy: 0.7415 - val_loss: 1.0469 - val_accuracy: 0.7144\n",
            "Epoch 15/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9268 - accuracy: 0.7417\n",
            "Epoch 00015: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1793s 605ms/step - loss: 0.9268 - accuracy: 0.7417 - val_loss: 1.0497 - val_accuracy: 0.7151\n",
            "Epoch 16/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9237 - accuracy: 0.7426\n",
            "Epoch 00016: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1750s 590ms/step - loss: 0.9237 - accuracy: 0.7426 - val_loss: 1.0464 - val_accuracy: 0.7166\n",
            "Epoch 17/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9249 - accuracy: 0.7416\n",
            "Epoch 00017: val_loss did not improve from 1.04387\n",
            "2965/2965 [==============================] - 1685s 568ms/step - loss: 0.9249 - accuracy: 0.7416 - val_loss: 1.0454 - val_accuracy: 0.7178\n",
            "Epoch 18/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9252 - accuracy: 0.7419\n",
            "Epoch 00018: val_loss improved from 1.04387 to 1.03782, saving model to percobaan_kaggle_v3_checkpoint.hdf5\n",
            "2965/2965 [==============================] - 1686s 569ms/step - loss: 0.9252 - accuracy: 0.7419 - val_loss: 1.0378 - val_accuracy: 0.7169\n",
            "Epoch 19/20\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9236 - accuracy: 0.7425\n",
            "Epoch 00019: val_loss did not improve from 1.03782\n",
            "2965/2965 [==============================] - 1694s 571ms/step - loss: 0.9236 - accuracy: 0.7425 - val_loss: 1.0384 - val_accuracy: 0.7210\n",
            "Epoch 20/20\n",
            "1449/2965 [=============>................] - ETA: 13:02 - loss: 0.9258 - accuracy: 0.7431Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYJuzzAnwLrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save('percobaan_kaggle_v3_sgd_00005_095_40_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9zYeQbnIW-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow.keras.preprocessing.image import img_to_array\n",
        "#from tensorflow.keras.models import load_model\n",
        "\n",
        "#model = load_model('percobaan_kaggle_v3_sgd_00005_095_40_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awH_l-NFIlaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.00001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='percobaan_kaggle_v3_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1N8xYJHIpnc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "outputId": "15be1333-9698-47f0-eb4b-f416d6ce6405"
      },
      "source": [
        "epochs = 10\n",
        "history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9136 - accuracy: 0.7448\n",
            "Epoch 00001: val_loss improved from inf to 1.04067, saving model to percobaan_kaggle_v3_checkpoint.hdf5\n",
            "2965/2965 [==============================] - 1802s 608ms/step - loss: 0.9136 - accuracy: 0.7448 - val_loss: 1.0407 - val_accuracy: 0.7200\n",
            "Epoch 2/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.7459\n",
            "Epoch 00002: val_loss improved from 1.04067 to 1.02559, saving model to percobaan_kaggle_v3_checkpoint.hdf5\n",
            "2965/2965 [==============================] - 1846s 622ms/step - loss: 0.9121 - accuracy: 0.7459 - val_loss: 1.0256 - val_accuracy: 0.7244\n",
            "Epoch 3/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9101 - accuracy: 0.7462\n",
            "Epoch 00003: val_loss did not improve from 1.02559\n",
            "2965/2965 [==============================] - 1800s 607ms/step - loss: 0.9101 - accuracy: 0.7462 - val_loss: 1.0419 - val_accuracy: 0.7214\n",
            "Epoch 4/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.7474\n",
            "Epoch 00004: val_loss did not improve from 1.02559\n",
            "2965/2965 [==============================] - 1712s 577ms/step - loss: 0.9122 - accuracy: 0.7474 - val_loss: 1.0391 - val_accuracy: 0.7202\n",
            "Epoch 5/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9097 - accuracy: 0.7465\n",
            "Epoch 00005: val_loss did not improve from 1.02559\n",
            "2965/2965 [==============================] - 1711s 577ms/step - loss: 0.9097 - accuracy: 0.7465 - val_loss: 1.0411 - val_accuracy: 0.7190\n",
            "Epoch 6/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9125 - accuracy: 0.7471\n",
            "Epoch 00006: val_loss did not improve from 1.02559\n",
            "2965/2965 [==============================] - 1714s 578ms/step - loss: 0.9125 - accuracy: 0.7471 - val_loss: 1.0278 - val_accuracy: 0.7230\n",
            "Epoch 7/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.7475\n",
            "Epoch 00007: val_loss did not improve from 1.02559\n",
            "2965/2965 [==============================] - 1725s 582ms/step - loss: 0.9111 - accuracy: 0.7475 - val_loss: 1.0487 - val_accuracy: 0.7175\n",
            "Epoch 8/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.7471\n",
            "Epoch 00008: val_loss did not improve from 1.02559\n",
            "2965/2965 [==============================] - 1711s 577ms/step - loss: 0.9092 - accuracy: 0.7471 - val_loss: 1.0335 - val_accuracy: 0.7203\n",
            "Epoch 9/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.7470\n",
            "Epoch 00009: val_loss did not improve from 1.02559\n",
            "2965/2965 [==============================] - 1716s 579ms/step - loss: 0.9134 - accuracy: 0.7470 - val_loss: 1.0379 - val_accuracy: 0.7170\n",
            "Epoch 10/10\n",
            "2965/2965 [==============================] - ETA: 0s - loss: 0.9109 - accuracy: 0.7469\n",
            "Epoch 00010: val_loss improved from 1.02559 to 1.02506, saving model to percobaan_kaggle_v3_checkpoint.hdf5\n",
            "2965/2965 [==============================] - 1707s 576ms/step - loss: 0.9109 - accuracy: 0.7469 - val_loss: 1.0251 - val_accuracy: 0.7198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4D7cQQCItKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('resnetv2_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yPpqNHxMWBcT",
        "colab": {}
      },
      "source": [
        "#starting to fine tuning model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnf2cfZMORix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('resnetv2_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iioNRVLUO9qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt8HOL_8PDBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0d66f446-2867-41c3-c586-c710e2098d57"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_resnet_v2 (Model)  (None, 1536)              54336736  \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 42)                64554     \n",
            "=================================================================\n",
            "Total params: 54,401,290\n",
            "Trainable params: 54,340,746\n",
            "Non-trainable params: 60,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Likr9LdOVIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.000005\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='percobaan_resv2_train4_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oit3dkZf1zMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4ca3b007-5726-47a5-9157-3d6077db6890"
      },
      "source": [
        "epochs = 2\n",
        "history4 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.9071 - accuracy: 0.7446\n",
            "Epoch 00001: val_loss improved from inf to 0.90795, saving model to percobaan_resv2_train4_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1950s 740ms/step - loss: 0.9071 - accuracy: 0.7446 - val_loss: 0.9079 - val_accuracy: 0.7492\n",
            "Epoch 2/2\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.7927 - accuracy: 0.7734\n",
            "Epoch 00002: val_loss improved from 0.90795 to 0.87001, saving model to percobaan_resv2_train4_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1938s 735ms/step - loss: 0.7927 - accuracy: 0.7734 - val_loss: 0.8700 - val_accuracy: 0.7628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ivBm4UTlizb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('resnetv2_finetuning_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWDgqptIllGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#epochs = 10\n",
        "#history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBOFy5XQ1rLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "f142968c-21c5-4c2b-f58f-e7d9d0d4d7e4"
      },
      "source": [
        "epochs = 8\n",
        "history4 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.7199 - accuracy: 0.7921\n",
            "Epoch 00001: val_loss improved from 0.87001 to 0.84610, saving model to percobaan_resv2_train4_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1936s 735ms/step - loss: 0.7199 - accuracy: 0.7921 - val_loss: 0.8461 - val_accuracy: 0.7688\n",
            "Epoch 2/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.6615 - accuracy: 0.8092\n",
            "Epoch 00002: val_loss improved from 0.84610 to 0.84296, saving model to percobaan_resv2_train4_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1935s 734ms/step - loss: 0.6615 - accuracy: 0.8092 - val_loss: 0.8430 - val_accuracy: 0.7744\n",
            "Epoch 3/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.8209\n",
            "Epoch 00003: val_loss improved from 0.84296 to 0.83357, saving model to percobaan_resv2_train4_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1942s 737ms/step - loss: 0.6148 - accuracy: 0.8209 - val_loss: 0.8336 - val_accuracy: 0.7767\n",
            "Epoch 4/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.8317\n",
            "Epoch 00004: val_loss did not improve from 0.83357\n",
            "2636/2636 [==============================] - 1941s 736ms/step - loss: 0.5652 - accuracy: 0.8317 - val_loss: 0.8386 - val_accuracy: 0.7775\n",
            "Epoch 5/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.5209 - accuracy: 0.8443\n",
            "Epoch 00005: val_loss did not improve from 0.83357\n",
            "2636/2636 [==============================] - 1935s 734ms/step - loss: 0.5209 - accuracy: 0.8443 - val_loss: 0.8375 - val_accuracy: 0.7817\n",
            "Epoch 6/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.4822 - accuracy: 0.8544\n",
            "Epoch 00006: val_loss did not improve from 0.83357\n",
            "2636/2636 [==============================] - 1936s 735ms/step - loss: 0.4822 - accuracy: 0.8544 - val_loss: 0.8336 - val_accuracy: 0.7814\n",
            "Epoch 7/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.8651\n",
            "Epoch 00007: val_loss did not improve from 0.83357\n",
            "2636/2636 [==============================] - 1934s 734ms/step - loss: 0.4409 - accuracy: 0.8651 - val_loss: 0.8512 - val_accuracy: 0.7808\n",
            "Epoch 8/8\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.4026 - accuracy: 0.8753\n",
            "Epoch 00008: val_loss did not improve from 0.83357\n",
            "2636/2636 [==============================] - 1935s 734ms/step - loss: 0.4026 - accuracy: 0.8753 - val_loss: 0.8635 - val_accuracy: 0.7838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPnsE6Ho9zii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6b1e9cd6-6796-4f86-cb89-c35870922fad"
      },
      "source": [
        "epochs = 2\n",
        "history4 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9082\n",
            "Epoch 00001: val_loss improved from inf to 0.87623, saving model to percobaan_resv2_train4_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1937s 735ms/step - loss: 0.2914 - accuracy: 0.9082 - val_loss: 0.8762 - val_accuracy: 0.7909\n",
            "Epoch 2/2\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9086\n",
            "Epoch 00002: val_loss did not improve from 0.87623\n",
            "2636/2636 [==============================] - 1928s 731ms/step - loss: 0.2915 - accuracy: 0.9086 - val_loss: 0.8802 - val_accuracy: 0.7908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tr-rTz04Nqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('resnetv2_finetuning2_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK6BqsvLFqDR",
        "colab_type": "text"
      },
      "source": [
        "#DEFINE MODEL 2: NASNETLARGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCitQkH4Fq88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#checkpointer = ModelCheckpoint(filepath='percobaandensenet.hdf5', verbose=1, save_best_only=True)\n",
        "#base_model = Xception(weights='imagenet',input_shape=(224,224,3), include_top=False, pooling='avg')\n",
        "base_model = tf.keras.applications.NASNetLarge(\n",
        "    input_shape=None,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling='avg',\n",
        "    classes=1000,\n",
        ")\n",
        "#base_model = efn.EfficientNetB7(weights='imagenet',input_shape=(224,224,3),include_top=False,pooling='avg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFL614JMFzoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.trainable=False\n",
        "model_in = Input(shape=(331,331,3))\n",
        "x = base_model(model_in,training=False)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "model_out = layers.Dense(42,activation='softmax')(x)\n",
        "model = Model(model_in,model_out)\n",
        "model.summary()\n",
        "#model = Sequential()\n",
        "#model.add(base_model)\n",
        "#model.add(layers.Dense(420,activation='relu'))\n",
        "#model.add(layers.Dropout(0.2))\n",
        "#model.add(layers.Dense(42,activation='softmax'))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A90qqe00gloR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2c2b3d23-8888-4c12-abc1-ae710f03359d"
      },
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment = AUGMENTATIONS,\n",
        "        preprocess_input=None,\n",
        "        validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(331, 331),\n",
        "        batch_size=18,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(331, 331),\n",
        "        batch_size=18,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 84331 images belonging to 42 classes.\n",
            "Found 21061 images belonging to 42 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OPecPSuF56z",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN MODEL 2: NASNETLARGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYGOP-NGF9DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.01\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='nasnetlarge_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vu_wtQJwGP2B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "8a632218-9fa9-4adf-a9cc-25b22f451315"
      },
      "source": [
        "epochs = 5\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.2560 - accuracy: 0.6622\n",
            "Epoch 00001: val_loss improved from inf to 1.16724, saving model to nasnetlarge_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1851s 702ms/step - loss: 1.2560 - accuracy: 0.6622 - val_loss: 1.1672 - val_accuracy: 0.6876\n",
            "Epoch 2/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0797 - accuracy: 0.7046\n",
            "Epoch 00002: val_loss improved from 1.16724 to 1.13688, saving model to nasnetlarge_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1834s 696ms/step - loss: 1.0797 - accuracy: 0.7046 - val_loss: 1.1369 - val_accuracy: 0.6944\n",
            "Epoch 3/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0260 - accuracy: 0.7158\n",
            "Epoch 00003: val_loss improved from 1.13688 to 1.10202, saving model to nasnetlarge_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1831s 695ms/step - loss: 1.0260 - accuracy: 0.7158 - val_loss: 1.1020 - val_accuracy: 0.7019\n",
            "Epoch 4/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.9887 - accuracy: 0.7246\n",
            "Epoch 00004: val_loss improved from 1.10202 to 1.08298, saving model to nasnetlarge_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1831s 695ms/step - loss: 0.9887 - accuracy: 0.7246 - val_loss: 1.0830 - val_accuracy: 0.7145\n",
            "Epoch 5/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.9677 - accuracy: 0.7312\n",
            "Epoch 00005: val_loss did not improve from 1.08298\n",
            "2636/2636 [==============================] - 1830s 694ms/step - loss: 0.9677 - accuracy: 0.7312 - val_loss: 1.0869 - val_accuracy: 0.7136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzvGO6-C3skT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='nasnetlarge_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMQveo93BMb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "47f5160f-6a27-4a58-cc0a-75e7fb4aa9e3"
      },
      "source": [
        "epochs = 2\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4686/4686 [==============================] - ETA: 0s - loss: 0.8019 - accuracy: 0.7687\n",
            "Epoch 00001: val_loss improved from inf to 0.99587, saving model to nasnetlarge_train_checkpoint.hdf5\n",
            "4686/4686 [==============================] - 1910s 407ms/step - loss: 0.8019 - accuracy: 0.7687 - val_loss: 0.9959 - val_accuracy: 0.7308\n",
            "Epoch 2/2\n",
            "4686/4686 [==============================] - ETA: 0s - loss: 0.7999 - accuracy: 0.7717\n",
            "Epoch 00002: val_loss improved from 0.99587 to 0.98411, saving model to nasnetlarge_train_checkpoint.hdf5\n",
            "4686/4686 [==============================] - 1901s 406ms/step - loss: 0.7999 - accuracy: 0.7717 - val_loss: 0.9841 - val_accuracy: 0.7366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7AXexEDtGP2J",
        "colab": {}
      },
      "source": [
        "model.save('nasnetlarge_train3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Ri4Sk16xXZN",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('nasnetlarge_train3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZwD06K2xU7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRTDMOQUxckG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "e6ba8440-a125-487b-d4e9-f46852578a0e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 331, 331, 3)]     0         \n",
            "_________________________________________________________________\n",
            "NASNet (Model)               (None, 4032)              84916818  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4032)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 42)                169386    \n",
            "=================================================================\n",
            "Total params: 85,086,204\n",
            "Trainable params: 84,889,536\n",
            "Non-trainable params: 196,668\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVs1fuDLxedi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.0001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='nasnetlarge_train_checkpoint_finetune.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgCG4i2o8fO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9e780e73-db41-4c93-8a62-12bf92a9c77c"
      },
      "source": [
        "epochs = 1\n",
        "history8 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4686/4686 [==============================] - ETA: 0s - loss: 0.7260 - accuracy: 0.7898\n",
            "Epoch 00001: val_loss improved from inf to 0.86133, saving model to nasnetlarge_train_checkpoint_finetune.hdf5\n",
            "4686/4686 [==============================] - 8395s 2s/step - loss: 0.7260 - accuracy: 0.7898 - val_loss: 0.8613 - val_accuracy: 0.7710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUcFS0mz0dWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('nasnetlarge_train4_finetuning.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcT2HYllVU1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3951d95b-bc87-4aeb-ddcd-c21dfab7c5f6"
      },
      "source": [
        "epochs = 1\n",
        "history8 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4686/4686 [==============================] - ETA: 0s - loss: 0.5928 - accuracy: 0.8272\n",
            "Epoch 00001: val_loss improved from 0.86133 to 0.84497, saving model to nasnetlarge_train_checkpoint_finetune.hdf5\n",
            "4686/4686 [==============================] - 8394s 2s/step - loss: 0.5928 - accuracy: 0.8272 - val_loss: 0.8450 - val_accuracy: 0.7774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB6MVRPsyP1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('nasnetlarge_train_finetuning2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLlIw-tFzXLW",
        "colab_type": "text"
      },
      "source": [
        "#DEFINE MODEL 3: EFFICIENTNET B7 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPmrA11axQpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "81f41ff0-7c98-4c88-e61d-fa0ec7b3ca34"
      },
      "source": [
        "pip install -U efficientnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet\n",
            "  Downloading https://files.pythonhosted.org/packages/28/91/67848a143b54c331605bfba5fd31cf4e9db13d2e429d103fe807acc3bcf4/efficientnet-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.12.0)\n",
            "Installing collected packages: efficientnet\n",
            "Successfully installed efficientnet-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v622gVNDFbaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ead7f4a3-291b-4f29-ef73-62a4ebb6ab11"
      },
      "source": [
        "import efficientnet.tfkeras\n",
        "\n",
        "base_model = efficientnet.tfkeras.EfficientNetB7(\n",
        "    input_shape=None,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling='avg',\n",
        "    classes=1000,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
            "258441216/258434480 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIBfs9UpFgoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "9608515c-290c-4f06-94ee-8acc4804ba0f"
      },
      "source": [
        "base_model.trainable=False\n",
        "model_in = Input(shape=(299,299,3))\n",
        "x = base_model(model_in,training=False)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "model_out = layers.Dense(42,activation='softmax')(x)\n",
        "model = Model(model_in,model_out)\n",
        "model.summary()\n",
        "#model = Sequential()\n",
        "#model.add(base_model)\n",
        "#model.add(layers.Dense(420,activation='relu'))\n",
        "#model.add(layers.Dropout(0.2))\n",
        "#model.add(layers.Dense(42,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "efficientnet-b7 (Model)      (None, 2560)              64097680  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2560)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 42)                107562    \n",
            "=================================================================\n",
            "Total params: 64,205,242\n",
            "Trainable params: 107,562\n",
            "Non-trainable params: 64,097,680\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3KAmCwkeHd42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0fc16aa8-2370-48cf-f14e-8a6bcd1f9f73"
      },
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment = AUGMENTATIONS,\n",
        "        preprocess_input=None,\n",
        "        validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 84331 images belonging to 42 classes.\n",
            "Found 21061 images belonging to 42 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuHftuh_HIlU",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN MODEL 3: EFFICIENTNET B7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-pXZ0IoVN9ZM",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.01\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb7_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bkvkPPVN9ZU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "7a1ca5df-70e2-410b-e9ad-52242e2f74bc"
      },
      "source": [
        "epochs = 5\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.3249 - accuracy: 0.6447\n",
            "Epoch 00001: val_loss improved from inf to 1.17280, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1707s 647ms/step - loss: 1.3249 - accuracy: 0.6447 - val_loss: 1.1728 - val_accuracy: 0.6830\n",
            "Epoch 2/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.1353 - accuracy: 0.6906\n",
            "Epoch 00002: val_loss improved from 1.17280 to 1.13890, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1629s 618ms/step - loss: 1.1353 - accuracy: 0.6906 - val_loss: 1.1389 - val_accuracy: 0.6935\n",
            "Epoch 3/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0894 - accuracy: 0.7008\n",
            "Epoch 00003: val_loss improved from 1.13890 to 1.11554, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1577s 598ms/step - loss: 1.0894 - accuracy: 0.7008 - val_loss: 1.1155 - val_accuracy: 0.6972\n",
            "Epoch 4/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0547 - accuracy: 0.7111\n",
            "Epoch 00004: val_loss improved from 1.11554 to 1.10598, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1607s 610ms/step - loss: 1.0547 - accuracy: 0.7111 - val_loss: 1.1060 - val_accuracy: 0.6986\n",
            "Epoch 5/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.7154\n",
            "Epoch 00005: val_loss improved from 1.10598 to 1.09322, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1600s 607ms/step - loss: 1.0343 - accuracy: 0.7154 - val_loss: 1.0932 - val_accuracy: 0.7031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BU8sPGRCm3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('efb7_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pba_KmZVcVgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.0001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb7_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NMg0h0MBT_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d5b332c2-8270-4c9e-c08a-53e7de00a84e"
      },
      "source": [
        "epochs = 2\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "7028/7028 [==============================] - ETA: 0s - loss: 0.9578 - accuracy: 0.7353\n",
            "Epoch 00001: val_loss improved from inf to 1.05413, saving model to efb7_train_checkpoint.hdf5\n",
            "7028/7028 [==============================] - 1725s 245ms/step - loss: 0.9578 - accuracy: 0.7353 - val_loss: 1.0541 - val_accuracy: 0.7159\n",
            "Epoch 2/2\n",
            "7028/7028 [==============================] - ETA: 0s - loss: 0.9555 - accuracy: 0.7352\n",
            "Epoch 00002: val_loss did not improve from 1.05413\n",
            "7028/7028 [==============================] - 1676s 238ms/step - loss: 0.9555 - accuracy: 0.7352 - val_loss: 1.0563 - val_accuracy: 0.7132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-PKN2FnAN9Zd",
        "colab": {}
      },
      "source": [
        "model.save('efb7_train3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dL3rlzX6N9Zf",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('efb7_train3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fXZ6n1cgN9Zh",
        "colab": {}
      },
      "source": [
        "model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNjB3ZhIN9Zj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "84d093da-8643-4f32-ab7d-0feee1e1d89e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "efficientnet-b7 (Model)      (None, 2560)              64097680  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2560)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 42)                107562    \n",
            "=================================================================\n",
            "Total params: 64,205,242\n",
            "Trainable params: 63,894,522\n",
            "Non-trainable params: 310,720\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VISQcJOwN9Zl",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.00001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb7_train_finetune_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qYieDafVLZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6b1e5f16-9dc0-41e8-bff6-1295f2d93eda"
      },
      "source": [
        "epochs = 1\n",
        "history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8434/8434 [==============================] - ETA: 0s - loss: 0.8735 - accuracy: 0.7584\n",
            "Epoch 00001: val_loss improved from inf to 0.95479, saving model to efb7_train_finetune_checkpoint.hdf5\n",
            "8434/8434 [==============================] - 6791s 805ms/step - loss: 0.8735 - accuracy: 0.7584 - val_loss: 0.9548 - val_accuracy: 0.7456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSVT7vUV4GDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('efb7_train_finetune.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5XywDfJVd5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "76aa1633-21cb-4c85-fc2b-3b0bbc8ead84"
      },
      "source": [
        "epochs = 1\n",
        "history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8434/8434 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.7709\n",
            "Epoch 00001: val_loss improved from 0.95479 to 0.92542, saving model to efb7_train_finetune_checkpoint.hdf5\n",
            "8434/8434 [==============================] - 6781s 804ms/step - loss: 0.8268 - accuracy: 0.7709 - val_loss: 0.9254 - val_accuracy: 0.7518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB6m0Tyu6lQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('efb7_train_finetune2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr-F5d4OyYr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('efb7_train_finetune2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CcGpumeyFo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.000001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb7_train_finetune_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVKN2ymylggC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "52bef267-6606-4bac-b931-1135b4c82227"
      },
      "source": [
        "epochs = 1\n",
        "history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8434/8434 [==============================] - ETA: 0s - loss: 0.8005 - accuracy: 0.7778\n",
            "Epoch 00001: val_loss improved from inf to 0.92294, saving model to efb7_train_finetune_checkpoint.hdf5\n",
            "8434/8434 [==============================] - 6771s 803ms/step - loss: 0.8005 - accuracy: 0.7778 - val_loss: 0.9229 - val_accuracy: 0.7524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azKJuRFNuJoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('efb7_train_finetune3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqf-ZjA2OOsG",
        "colab_type": "text"
      },
      "source": [
        "#DEFINE MODEL 4: EFFICIENTNETB6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hr7fcwBdO6CD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "3884a002-e52f-413e-b739-423172577cd5"
      },
      "source": [
        "pip install -U efficientnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet\n",
            "  Downloading https://files.pythonhosted.org/packages/28/91/67848a143b54c331605bfba5fd31cf4e9db13d2e429d103fe807acc3bcf4/efficientnet-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.12.0)\n",
            "Installing collected packages: efficientnet\n",
            "Successfully installed efficientnet-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xzd4yRLZO6CI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "69a81cbb-b277-4381-ce66-9dd98673c3c1"
      },
      "source": [
        "import efficientnet.tfkeras\n",
        "\n",
        "base_model = efficientnet.tfkeras.EfficientNetB6(\n",
        "    input_shape=None,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling='avg',\n",
        "    classes=1000,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
            "165527552/165527152 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "leMHhhZbO6CK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "3f746b64-e648-4045-e14f-1b71d8830632"
      },
      "source": [
        "base_model.trainable=False\n",
        "model_in = Input(shape=(299,299,3))\n",
        "x = base_model(model_in,training=False)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "model_out = layers.Dense(42,activation='softmax')(x)\n",
        "model = Model(model_in,model_out)\n",
        "model.summary()\n",
        "#model = Sequential()\n",
        "#model.add(base_model)\n",
        "#model.add(layers.Dense(420,activation='relu'))\n",
        "#model.add(layers.Dropout(0.2))\n",
        "#model.add(layers.Dense(42,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "efficientnet-b6 (Model)      (None, 2304)              40960136  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 42)                96810     \n",
            "=================================================================\n",
            "Total params: 41,056,946\n",
            "Trainable params: 96,810\n",
            "Non-trainable params: 40,960,136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wom0Vw5wO6CN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ee794b1-0e17-4546-8b95-834d94843343"
      },
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment = AUGMENTATIONS,\n",
        "        preprocess_input=None,\n",
        "        validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17630 images belonging to 9 classes.\n",
            "Found 4403 images belonging to 9 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNeYmMclO9hr",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN MODEL 4: EFFICIENTNET B6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YJkLMAeBO6CP",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.01\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb7_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UnZ06DKVO6CQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "b08ceb02-207a-40f4-98b3-166c238481a0"
      },
      "source": [
        "epochs = 5\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.3788 - accuracy: 0.6318\n",
            "Epoch 00001: val_loss improved from inf to 1.22006, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1485s 563ms/step - loss: 1.3788 - accuracy: 0.6318 - val_loss: 1.2201 - val_accuracy: 0.6724\n",
            "Epoch 2/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.1821 - accuracy: 0.6773\n",
            "Epoch 00002: val_loss improved from 1.22006 to 1.17889, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1443s 547ms/step - loss: 1.1821 - accuracy: 0.6773 - val_loss: 1.1789 - val_accuracy: 0.6814\n",
            "Epoch 3/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.1321 - accuracy: 0.6871\n",
            "Epoch 00003: val_loss improved from 1.17889 to 1.15241, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1428s 542ms/step - loss: 1.1321 - accuracy: 0.6871 - val_loss: 1.1524 - val_accuracy: 0.6870\n",
            "Epoch 4/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.1057 - accuracy: 0.6958\n",
            "Epoch 00004: val_loss improved from 1.15241 to 1.13484, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1427s 541ms/step - loss: 1.1057 - accuracy: 0.6958 - val_loss: 1.1348 - val_accuracy: 0.6933\n",
            "Epoch 5/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0826 - accuracy: 0.7006\n",
            "Epoch 00005: val_loss improved from 1.13484 to 1.13178, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1416s 537ms/step - loss: 1.0826 - accuracy: 0.7006 - val_loss: 1.1318 - val_accuracy: 0.6929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a14bT6taO6CS",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb6_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkohEOnUcXX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "e8b22151-0c5e-4743-c5a7-8df4508ee519"
      },
      "source": [
        "epochs = 5\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.7148\n",
            "Epoch 00001: val_loss improved from inf to 1.10633, saving model to efb6_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1446s 549ms/step - loss: 1.0351 - accuracy: 0.7148 - val_loss: 1.1063 - val_accuracy: 0.7027\n",
            "Epoch 2/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0271 - accuracy: 0.7188\n",
            "Epoch 00002: val_loss improved from 1.10633 to 1.10281, saving model to efb6_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1425s 541ms/step - loss: 1.0271 - accuracy: 0.7188 - val_loss: 1.1028 - val_accuracy: 0.7032\n",
            "Epoch 3/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0200 - accuracy: 0.7195\n",
            "Epoch 00003: val_loss improved from 1.10281 to 1.09706, saving model to efb6_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 1427s 541ms/step - loss: 1.0200 - accuracy: 0.7195 - val_loss: 1.0971 - val_accuracy: 0.7054\n",
            "Epoch 4/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0225 - accuracy: 0.7196\n",
            "Epoch 00004: val_loss did not improve from 1.09706\n",
            "2636/2636 [==============================] - 1418s 538ms/step - loss: 1.0225 - accuracy: 0.7196 - val_loss: 1.0996 - val_accuracy: 0.7050\n",
            "Epoch 5/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 1.0156 - accuracy: 0.7210\n",
            "Epoch 00005: val_loss did not improve from 1.09706\n",
            "2636/2636 [==============================] - 1415s 537ms/step - loss: 1.0156 - accuracy: 0.7210 - val_loss: 1.0976 - val_accuracy: 0.7042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SghgjZy8zoSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.0001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb7_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nEr2Z05zqdC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "14629dbb-a456-40cf-8a95-a8854d2d23a8"
      },
      "source": [
        "epochs = 5\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.8989 - accuracy: 0.7504\n",
            "Epoch 00001: val_loss improved from inf to 1.04043, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 2297s 871ms/step - loss: 0.8989 - accuracy: 0.7504 - val_loss: 1.0404 - val_accuracy: 0.7199\n",
            "Epoch 2/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.8985 - accuracy: 0.7501\n",
            "Epoch 00002: val_loss improved from 1.04043 to 1.03745, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 2268s 860ms/step - loss: 0.8985 - accuracy: 0.7501 - val_loss: 1.0375 - val_accuracy: 0.7223\n",
            "Epoch 3/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.8991 - accuracy: 0.7493\n",
            "Epoch 00003: val_loss improved from 1.03745 to 1.03610, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 2281s 865ms/step - loss: 0.8991 - accuracy: 0.7493 - val_loss: 1.0361 - val_accuracy: 0.7232\n",
            "Epoch 4/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.9019 - accuracy: 0.7510\n",
            "Epoch 00004: val_loss improved from 1.03610 to 1.03153, saving model to efb7_train_checkpoint.hdf5\n",
            "2636/2636 [==============================] - 2273s 862ms/step - loss: 0.9019 - accuracy: 0.7510 - val_loss: 1.0315 - val_accuracy: 0.7219\n",
            "Epoch 5/5\n",
            "2636/2636 [==============================] - ETA: 0s - loss: 0.8976 - accuracy: 0.7509\n",
            "Epoch 00005: val_loss did not improve from 1.03153\n",
            "2636/2636 [==============================] - 2269s 861ms/step - loss: 0.8976 - accuracy: 0.7509 - val_loss: 1.0363 - val_accuracy: 0.7230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AAzrUupHBd29",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.0001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb6_train_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-9ygvnAyO6Cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "80e2e1f7-57a8-4cdd-b024-4da234c52620"
      },
      "source": [
        "epochs = 3\n",
        "history = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7028/7028 [==============================] - ETA: 0s - loss: 1.0094 - accuracy: 0.7215\n",
            "Epoch 00001: val_loss improved from inf to 1.10049, saving model to efb6_train_checkpoint.hdf5\n",
            "7028/7028 [==============================] - 1522s 217ms/step - loss: 1.0094 - accuracy: 0.7215 - val_loss: 1.1005 - val_accuracy: 0.7022\n",
            "Epoch 2/3\n",
            "7028/7028 [==============================] - ETA: 0s - loss: 1.0084 - accuracy: 0.7226\n",
            "Epoch 00002: val_loss did not improve from 1.10049\n",
            "7028/7028 [==============================] - 1489s 212ms/step - loss: 1.0084 - accuracy: 0.7226 - val_loss: 1.1012 - val_accuracy: 0.7028\n",
            "Epoch 3/3\n",
            "7028/7028 [==============================] - ETA: 0s - loss: 1.0081 - accuracy: 0.7233\n",
            "Epoch 00003: val_loss improved from 1.10049 to 1.09909, saving model to efb6_train_checkpoint.hdf5\n",
            "7028/7028 [==============================] - 1484s 211ms/step - loss: 1.0081 - accuracy: 0.7233 - val_loss: 1.0991 - val_accuracy: 0.7044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yOWWijRvO6Cc",
        "colab": {}
      },
      "source": [
        "model.save('efb6_train2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bi8WTRNqO6Ce",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('efb6_train2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3EBL9dNxO6Ch",
        "colab": {}
      },
      "source": [
        "model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-HSzKf5O6Ci",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "7b681188-bb30-437f-848d-4a33b228441a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "efficientnet-b6 (Model)      (None, 2304)              40960136  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 42)                96810     \n",
            "=================================================================\n",
            "Total params: 41,056,946\n",
            "Trainable params: 40,832,514\n",
            "Non-trainable params: 224,432\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8jOZmc5gMsk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5683fcfd-dcc0-425c-93cc-4dde15e5f367"
      },
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment = AUGMENTATIONS,\n",
        "        preprocess_input=None,\n",
        "        validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=10,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=10,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 84331 images belonging to 42 classes.\n",
            "Found 21061 images belonging to 42 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NaDGBu8_O6Cn",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.00001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb6_train_finetune_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gRCZBD_O6Cp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9ee58fa4-92f3-40dc-b1d6-33ceed5f7724"
      },
      "source": [
        "epochs = 1\n",
        "history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8434/8434 [==============================] - ETA: 0s - loss: 0.7187 - accuracy: 0.8000\n",
            "Epoch 00001: val_loss improved from inf to 0.89980, saving model to efb6_train_finetune_checkpoint.hdf5\n",
            "8434/8434 [==============================] - 6932s 822ms/step - loss: 0.7187 - accuracy: 0.8000 - val_loss: 0.8998 - val_accuracy: 0.7611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xrtsIFiuO6Cq",
        "colab": {}
      },
      "source": [
        "model.save('efb6_train_finetune2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXtuNJFI4vTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#param\n",
        "LR=0.000001\n",
        "momen=0.95\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='efb6_train_finetune_checkpoint.hdf5', verbose=1, save_best_only=True)\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "    learning_rate=LR,\n",
        "    momentum=momen,\n",
        "    name=\"SGD\",\n",
        "    ),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=False,\n",
        "        label_smoothing=0,\n",
        "        reduction=\"auto\",\n",
        "        name=\"categorical_crossentropy\"),\n",
        "    metrics=['accuracy'])\n",
        "    #finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGfkQSq3ViiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1b1c0a40-28c7-437d-efbd-786fc286514b"
      },
      "source": [
        "epochs = 1\n",
        "history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8434/8434 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.8061\n",
            "Epoch 00001: val_loss improved from inf to 0.90016, saving model to efb6_train_finetune_checkpoint.hdf5\n",
            "8434/8434 [==============================] - 6910s 819ms/step - loss: 0.6983 - accuracy: 0.8061 - val_loss: 0.9002 - val_accuracy: 0.7579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvHYVs6I5A8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d180c984-d68c-4de7-a8dd-d2685853cb53"
      },
      "source": [
        "epochs = 2\n",
        "history2 = model.fit(train_generator, epochs=epochs, callbacks =[checkpointer], validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "8434/8434 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.8078\n",
            "Epoch 00001: val_loss improved from 0.90016 to 0.89328, saving model to efb6_train_finetune_checkpoint.hdf5\n",
            "8434/8434 [==============================] - 6903s 818ms/step - loss: 0.6937 - accuracy: 0.8078 - val_loss: 0.8933 - val_accuracy: 0.7593\n",
            "Epoch 2/2\n",
            "8434/8434 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.8066\n",
            "Epoch 00002: val_loss did not improve from 0.89328\n",
            "8434/8434 [==============================] - 6895s 818ms/step - loss: 0.6967 - accuracy: 0.8066 - val_loss: 0.8940 - val_accuracy: 0.7630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYw1cA4y5FEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('efb6_train_finetune3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq5EKv5TP31t",
        "colab_type": "text"
      },
      "source": [
        "#ENSEMBLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx3gTc0GsB4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import dstack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEVXeHvSsB4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv('test.csv')#load path image data test\n",
        "test_df = test_df.astype('str', copy=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrHGTkDyDfxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('/content/train.csv')#load path image dan kategori data train\n",
        "train_df = train_df.astype('str', copy=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egw0zOrATxWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "outputId": "c90c1458-6132-42a8-8635-de88a4523fc3"
      },
      "source": [
        "!pip install deepstack==0.0.9\n",
        "#installing deepstack ensemble library"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepstack==0.0.9\n",
            "  Downloading https://files.pythonhosted.org/packages/36/0a/7555b16579570cad2ec2b02b7a52ae6406f983e8fdde156ac3fe109fd16f/deepstack-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from deepstack==0.0.9) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.6/dist-packages (from deepstack==0.0.9) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from deepstack==0.0.9) (2.2.0)\n",
            "Requirement already satisfied: keras>=2.2.5 in /usr/local/lib/python3.6/dist-packages (from deepstack==0.0.9) (2.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.2->deepstack==0.0.9) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.2->deepstack==0.0.9) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (3.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (1.30.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (2.2.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (0.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (0.34.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->deepstack==0.0.9) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.5->deepstack==0.0.9) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.5->deepstack==0.0.9) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=1.14.0->deepstack==0.0.9) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (1.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (0.2.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14.0->deepstack==0.0.9) (3.1.0)\n",
            "Installing collected packages: deepstack\n",
            "Successfully installed deepstack-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTmbuCJIbGiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "083cf7ef-cc69-4256-af99-a7d3b6141183"
      },
      "source": [
        "cek = '/content/test/test'#test image data folder path\n",
        "\n",
        "AUGMENTATIONS = albumentations.Compose([\n",
        "    #albumentations.Transpose(p=0.5),\n",
        "    albumentations.HorizontalFlip(p=0.5),\n",
        "    albumentations.CLAHE(),\n",
        "    albumentations.Cutout(num_holes=2,max_h_size=70,max_w_size=70,p=0.2),\n",
        "    albumentations.OneOf([\n",
        "    albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n",
        "    albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n",
        "    ],p=1),\n",
        "    albumentations.Blur(p=0.05),\n",
        "    albumentations.HueSaturationValue(p=0.5),\n",
        "    albumentations.RGBShift(p=0.5),\n",
        "    albumentations.Rotate(limit=15),\n",
        "])\n",
        "\n",
        "albu_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment = AUGMENTATIONS,\n",
        "        preprocess_input=None,\n",
        "        #validation_split=0.1)\n",
        ")\n",
        "\n",
        "test_albu = albu_datagen.flow_from_dataframe(\n",
        "    dataframe = test_df,\n",
        "    directory = cek,\n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    #has_ext=False,\n",
        "    subset='training',\n",
        "    target_size = (299,299),\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    class_mode = 'categorical',\n",
        "    shuffle=False,\n",
        "    validate_filenames = True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12186 validated image filenames belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh_4X3NkeROR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a36a18a0-e4a1-42d6-a56a-7aaacc278c9d"
      },
      "source": [
        "train_datagen = ImageDataAugmentor(\n",
        "        rescale=1./255,\n",
        "        augment = AUGMENTATIONS,\n",
        "        preprocess_input=None,\n",
        "        validation_split=0.2)\n",
        "\n",
        "train_generator_albu = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        subset='training')\n",
        "\n",
        "validation_generator_albu = train_datagen.flow_from_directory(\n",
        "        '/content/train/train',\n",
        "        target_size=(299, 299),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 84331 images belonging to 42 classes.\n",
            "Found 21061 images belonging to 42 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q3WtAR0xeRqA",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#load model('model path')\n",
        "model1 = load_model('resnetv2_finetuning2_train.h5')\n",
        "model2 = load_model('nasnetlarge_train_finetuning2.h5')\n",
        "model3 = load_model('efb7_train_finetune3.h5')\n",
        "model4 = load_model('efb6_train_finetune3.h5')\n",
        "#model4 = ...\n",
        "#etc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XPW2EkPueRqC",
        "colab": {}
      },
      "source": [
        "from deepstack.base import KerasMember  # For a generic (i.e., Non-Keras Model) check the class `Member`\n",
        "from deepstack.ensemble import StackEnsemble"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qqpa1i5VeRqG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "97e72c5f-393b-4c29-ba63-639ba59e6c63"
      },
      "source": [
        "member1 = KerasMember(name='model1',keras_model=model1,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "\"\"\"member11 = KerasMember(name='model1',keras_model=model1,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member12 = KerasMember(name='model1',keras_model=model1,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member13 = KerasMember(name='model1',keras_model=model1,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member14 = KerasMember(name='model1',keras_model=model1,train_batches=train_generator_albu,val_batches=validation_generator_albu)\"\"\"\n",
        "\n",
        "member2 = KerasMember(name='model2',keras_model=model2,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "\"\"\"member21 = KerasMember(name='model2',keras_model=model2,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member22 = KerasMember(name='model2',keras_model=model2,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member23 = KerasMember(name='model2',keras_model=model2,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member24 = KerasMember(name='model2',keras_model=model2,train_batches=train_generator_albu,val_batches=validation_generator_albu)\"\"\"\n",
        "\n",
        "\n",
        "member3 = KerasMember(name='model3',keras_model=model3,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "\"\"\"member31 = KerasMember(name='model3',keras_model=model3,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member32 = KerasMember(name='model3',keras_model=model3,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member33 = KerasMember(name='model3',keras_model=model3,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member34 = KerasMember(name='model3',keras_model=model3,train_batches=train_generator_albu,val_batches=validation_generator_albu)\"\"\"\n",
        "\n",
        "member4 = KerasMember(name='model4',keras_model=model4,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "\"\"\"member41 = KerasMember(name='model4',keras_model=model4,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member42 = KerasMember(name='model4',keras_model=model4,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member43 = KerasMember(name='model4',keras_model=model4,train_batches=train_generator_albu,val_batches=validation_generator_albu)\n",
        "member44 = KerasMember(name='model4',keras_model=model4,train_batches=train_generator_albu,val_batches=validation_generator_albu)\"\"\"\n",
        "\n",
        "#member4 = KerasMember(...)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deepstack/base.py:148: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.predict, which supports generators.\n",
            "2636/2636 [==============================] - 1142s 433ms/step\n",
            "659/659 [==============================] - 284s 430ms/step\n",
            "2636/2636 [==============================] - 1116s 423ms/step\n",
            "659/659 [==============================] - 282s 427ms/step\n",
            "2636/2636 [==============================] - 1130s 429ms/step\n",
            "659/659 [==============================] - 282s 429ms/step\n",
            "2636/2636 [==============================] - 1126s 427ms/step\n",
            "659/659 [==============================] - 281s 427ms/step\n",
            "2636/2636 [==============================] - 1129s 428ms/step\n",
            "659/659 [==============================] - 283s 429ms/step\n",
            "2636/2636 [==============================] - 1243s 472ms/step\n",
            "659/659 [==============================] - 310s 471ms/step\n",
            "2636/2636 [==============================] - 1244s 472ms/step\n",
            "659/659 [==============================] - 310s 470ms/step\n",
            "2636/2636 [==============================] - 1243s 472ms/step\n",
            "659/659 [==============================] - 310s 471ms/step\n",
            "2636/2636 [==============================] - 1243s 472ms/step\n",
            "659/659 [==============================] - 311s 471ms/step\n",
            "2636/2636 [==============================] - 1244s 472ms/step\n",
            "659/659 [==============================] - 310s 470ms/step\n",
            "2636/2636 [==============================] - 1170s 444ms/step\n",
            "659/659 [==============================] - 291s 442ms/step\n",
            "2636/2636 [==============================] - 1168s 443ms/step\n",
            "659/659 [==============================] - 292s 443ms/step\n",
            "2636/2636 [==============================] - 1165s 442ms/step\n",
            "659/659 [==============================] - 292s 443ms/step\n",
            "2636/2636 [==============================] - 1170s 444ms/step\n",
            "659/659 [==============================] - 294s 446ms/step\n",
            "2636/2636 [==============================] - 1172s 445ms/step\n",
            "659/659 [==============================] - 293s 444ms/step\n",
            "2636/2636 [==============================] - 1173s 445ms/step\n",
            "659/659 [==============================] - 292s 443ms/step\n",
            "2636/2636 [==============================] - 1171s 444ms/step\n",
            "659/659 [==============================] - 291s 442ms/step\n",
            "2636/2636 [==============================] - 1170s 444ms/step\n",
            "659/659 [==============================] - 292s 444ms/step\n",
            "2636/2636 [==============================] - 1183s 449ms/step\n",
            "659/659 [==============================] - 295s 448ms/step\n",
            "2636/2636 [==============================] - 1174s 445ms/step\n",
            "659/659 [==============================] - 292s 443ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKyR4UUIfowQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cefd9951-0b79-4c15-9fb9-a90182411253"
      },
      "source": [
        "#Because it was on the edge of the end, we dont use any k fold search to find the best classifier or its param, we just use trial and error method\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from deepstack.ensemble import StackEnsemble\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from deepstack.ensemble import StackEnsemble\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "stack = StackEnsemble()\n",
        "\n",
        "\"\"\"# 2nd Level Meta-Learner\n",
        "estimators = [\n",
        "    #('rf', RandomForestClassifier(verbose=0, n_estimators=200, max_depth=15, n_jobs=20, min_samples_split=30)),\n",
        "    ('etr', ExtraTreesClassifier(verbose=0, n_estimators=200, max_depth=10, n_jobs=20, min_samples_split=20))\n",
        "]\n",
        "# 3rd Level Meta-Learner\n",
        "clf = StackingClassifier(\n",
        "    estimators=estimators, final_estimator=make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5))\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"stack = StackEnsemble()\n",
        "stack.model = RandomForestRegressor(verbose=0, n_estimators=200, \n",
        "                                  max_depth=15, n_jobs=20, min_samples_split=20)\"\"\"\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=62)\n",
        "stack = StackEnsemble()\n",
        "#stack.model = LogisticRegression(random_state=0,n_jobs=4,verbose=1) #this for logistic regressor meta learner\n",
        "clf = make_pipeline(StandardScaler(),knn) #this for SVC meta-learner\n",
        "stack.model = clf\n",
        "#stack.add_members([member1, member2, member3,member4])\n",
        "stack.add_members([member1, member2, member3, member4])\n",
        "#stack.add_members([member12, member22, member32,member42])\n",
        "#stack.add_members([member13, member23, member33,member43])\n",
        "#stack.add_members([member14, member24, member34,member44])\n",
        "\n",
        "#stack.add_members([member1, member11, member12,member13,member14, member2, member21, member22, member23, member24, member3, member31, member32, member33, member34, member4, member41, member42, member43, member44])\n",
        "# pake stack.add_members([member1, member2, member3,member4,member5,...]) sesuai jumlah member dari ensemblenya\n",
        "stack.fit()\n",
        "stack.describe(metric = accuracy_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling predict\n",
            "model1 - accuracy_score: 0.9856\n",
            "model2 - accuracy_score: 0.9839\n",
            "model3 - accuracy_score: 0.9815\n",
            "model4 - accuracy_score: 0.9827\n",
            "StackEnsemble accuracy_score: 0.8103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8103129006220028"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjPpCMKjeRqK",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "po0SUv69eRqP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "d90c6ee6-2230-4486-8095-a274010498c2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tta_steps = 10\n",
        "\n",
        "predictions1 = []\n",
        "\n",
        "for i in tqdm(range(tta_steps)):\n",
        "    print(i)\n",
        "    preds = member1._calculate_predictions(test_image)\n",
        "    predictions1.append(preds)\n",
        "\n",
        "prediction1 = np.mean(predictions1, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "381/381 [==============================] - 99s 261ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [01:40<15:00, 100.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "381/381 [==============================] - 99s 261ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [03:20<13:20, 100.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "381/381 [==============================] - 99s 260ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [04:59<11:39, 99.91s/it] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "381/381 [==============================] - 98s 257ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [06:38<09:57, 99.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "381/381 [==============================] - 98s 258ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [08:17<08:16, 99.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "381/381 [==============================] - 98s 258ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [09:56<06:36, 99.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "381/381 [==============================] - 98s 258ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [11:35<04:57, 99.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "381/381 [==============================] - 98s 257ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [13:13<03:17, 98.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "381/381 [==============================] - 99s 261ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [14:53<01:39, 99.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "381/381 [==============================] - 97s 254ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [16:31<00:00, 99.13s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3-fP_AFneRqS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "010cf6ac-9d12-44e3-cded-6b1b5db0b059"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tta_steps = 10\n",
        "\n",
        "predictions2 = []\n",
        "\n",
        "for i in tqdm(range(tta_steps)):\n",
        "    print(i)\n",
        "    preds = member2._calculate_predictions(test_image)\n",
        "    predictions2.append(preds)\n",
        "\n",
        "prediction2 = np.mean(predictions2, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [02:58<26:49, 178.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [05:57<23:50, 178.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [08:56<20:51, 178.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [11:55<17:52, 178.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [14:54<14:54, 178.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [17:52<11:55, 178.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [20:51<08:56, 178.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [23:50<05:57, 178.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [26:49<02:58, 178.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "381/381 [==============================] - 178s 467ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [29:48<00:00, 178.83s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qONqFFJ7eRqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "e55e7bc9-0deb-4244-d4c8-d20f3159c9d5"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tta_steps = 10\n",
        "\n",
        "predictions3 = []\n",
        "\n",
        "for i in tqdm(range(tta_steps)):\n",
        "    print(i)\n",
        "    preds = member3._calculate_predictions(test_image)\n",
        "    predictions3.append(preds)\n",
        "\n",
        "prediction3 = np.mean(predictions3, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "381/381 [==============================] - 150s 394ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [02:30<22:37, 150.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [05:01<20:05, 150.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [07:31<17:34, 150.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [10:01<15:03, 150.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [12:32<12:32, 150.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [15:02<10:01, 150.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [17:32<07:31, 150.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [20:03<05:00, 150.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [22:33<02:30, 150.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [25:03<00:00, 150.37s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PxZrK6wbeRqW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "4a26f875-f8be-41a4-d949-cbd38221b440"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tta_steps = 10\n",
        "\n",
        "predictions4 = []\n",
        "\n",
        "for i in tqdm(range(tta_steps)):\n",
        "    print(i)\n",
        "    preds = member4._calculate_predictions(test_image)\n",
        "    predictions4.append(preds)\n",
        "\n",
        "prediction4 = np.mean(predictions4, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1/10 [02:30<22:32, 150.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [05:00<20:02, 150.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [07:30<17:31, 150.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [10:01<15:01, 150.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [12:31<12:31, 150.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [15:01<10:01, 150.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [17:32<07:30, 150.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [20:02<05:00, 150.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "381/381 [==============================] - 150s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [22:32<02:30, 150.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "381/381 [==============================] - 149s 392ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [25:03<00:00, 150.31s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o4INNSFfeRqY",
        "colab": {}
      },
      "source": [
        "prediction_arr = np.hstack((prediction1,prediction2))\n",
        "prediction_arr = np.hstack((prediction_arr,prediction3))#hstack sesuai jumlah prediction (klo 4 member brarti ada 4 prediction yang harus di hstack)\n",
        "prediction_arr = np.hstack((prediction_arr,prediction4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tt4h4PIYeRqa",
        "colab": {}
      },
      "source": [
        "#predict_test = stack.predict(X=test_image)\n",
        "#predicting test data\n",
        "for index, row in test_df.iterrows():\n",
        "  predict_test_data = stack.predict(X=prediction_arr[index].reshape(1, -1))#stack_predict(stack,cek+'/'+row['filename'])\n",
        "  row['category'] = predict_test_data[0]#np.argmax(predict_test_data[index])\n",
        "\n",
        "test_df['category'] = test_df.category.apply(lambda c: str(c).zfill(2))\n",
        "test_df.to_csv('jawaban.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C3dKJyDueRqc",
        "colab": {}
      },
      "source": [
        "test_df.head()\n",
        "test_df.to_csv('jawaban.csv', index=False)\n",
        "#save submission"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}